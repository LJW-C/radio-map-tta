import torch
import torch.nn as nn
import torch.jit
import torch.optim as optim
from copy import deepcopy

class CoTTA(nn.Module):
    def __init__(self, model, steps=1, alpha=0.999, lr=0.0002):
        super().__init__()
        self.steps = steps
        self.alpha = alpha 
        self.base_lr = lr

        self.model, self.optimizer = prepare_model_and_optimizer(model, lr)
        self.model_ema = deepcopy(self.model)
        for param in self.model_ema.parameters():
            param.detach_() 

    def forward(self, inputs, target):
        for _ in range(self.steps):
            outputs = self.forward_and_adapt(inputs, target)
        return outputs

    @torch.enable_grad()
    def forward_and_adapt(self, inputs, target):
        with torch.no_grad():
            _, teacher_outputs = self.model_ema(inputs, return_features=True)
            
        _, student_outputs = self.model(inputs, return_features=True)
        loss_supervised = nn.MSELoss()(student_outputs, target)
        loss_consistency = nn.MSELoss()(student_outputs, teacher_outputs)
        loss = loss_supervised + 0.1 * loss_consistency 
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        self.update_ema_variables(self.model, self.model_ema, self.alpha)
        
        return student_outputs

    def update_ema_variables(self, model, ema_model, alpha):
        for param, ema_param in zip(model.parameters(), ema_model.parameters()):
            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)
            
    def reset(self):
        self.model_ema.load_state_dict(self.model.state_dict())

def collect_params(model):
    params = []
    names = []
    for nm, m in model.named_modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm, nn.Conv2d)):
            for np, p in m.named_parameters():
                if np in ['weight', 'bias']:
                    params.append(p)
                    names.append(f"{nm}.{np}")
    return params, names

def configure_model(model):
    model.train()
    model.requires_grad_(False)
    for m in model.modules():
        if isinstance(m, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):
            m.requires_grad_(True)
            m.track_running_stats = False 
        elif isinstance(m, nn.BatchNorm1d):
            m.train()
            m.requires_grad_(True)
        elif isinstance(m, nn.Conv2d):
            m.train()
            m.requires_grad_(True)
    return model

def prepare_model_and_optimizer(model, lr):
    model = configure_model(model)
    params, param_names = collect_params(model)
    optimizer = optim.Adam(params, lr=lr, betas=(0.9, 0.999))
    return model, optimizer

def setup(model, args):
    return CoTTA(model)
