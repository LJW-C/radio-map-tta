from copy import deepcopy
import torch
import torch.nn as nn
import torch.jit
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import numpy as np
import random
import logging

logger = logging.getLogger(__name__)

__all__ = ["setup"]

class TimelinessMemoryBank:
    def __init__(self, capacity):
        self.capacity = capacity
        self.data = deque(maxlen=capacity)
        self.counter = 0 

    def push(self, inputs, preds):
        batch_size = inputs.size(0)
        for i in range(batch_size):
            item = (
                inputs[i].detach().cpu(),  
                preds[i].detach().cpu(), 
                self.counter
            )
            self.data.append(item)
        self.counter += 1

    def sample(self, batch_size):
        if len(self.data) < batch_size:
            return None, None, None
        
        samples = random.sample(self.data, batch_size)
        
        inputs_list = []
        preds_list = []
        ages_list = []
        
        current_time = self.counter
        
        for (inp, pred, timestamp) in samples:
            inputs_list.append(inp)
            preds_list.append(pred)
            ages_list.append(current_time - timestamp)
            
        inputs = torch.stack(inputs_list).cuda() 
        preds = torch.stack(preds_list).cuda()
        ages = torch.tensor(ages_list, dtype=torch.float32).cuda()
        
        return inputs, preds, ages

def timeliness_reweighting(ages):
    return torch.exp(-ages * 0.01) 

class RoTTA(nn.Module):
    def __init__(self, model, lr=0.00002, alpha=0.999, memory_size=64, nu=0.5):
        super().__init__()
        self.lr = lr
        self.alpha = alpha
        self.memory_size = memory_size
        self.nu = nu 
      
        self.model, self.optimizer = prepare_model_and_optimizer(model, lr)
        

        self.model_ema = deepcopy(self.model)
        for param in self.model_ema.parameters():
            param.detach_()
            
        self.mem = TimelinessMemoryBank(capacity=memory_size)

    def forward(self, inputs, target=None):
        outputs = self.forward_and_adapt(inputs)
        return outputs

    @torch.enable_grad()
    def forward_and_adapt(self, inputs):
        with torch.no_grad():
            _, outputs_ema = self.model_ema(inputs, return_features=True)
            outputs_ema = outputs_ema.detach()

        _, outputs = self.model(inputs, return_features=True)
        loss_curr = nn.MSELoss()(outputs, outputs_ema)
        loss_replay = torch.tensor(0.0).to(inputs.device)
        
        self.mem.push(inputs, outputs_ema)
        mem_inputs, mem_preds, mem_ages = self.mem.sample(batch_size=inputs.size(0))
        
        if mem_inputs is not None:
            _, mem_outputs_student = self.model(mem_inputs, return_features=True)
            mse_per_sample = ((mem_outputs_student - mem_preds) ** 2).mean(dim=(1, 2, 3)) # 假设输出是 (B, C, H, W)
            
            weights = timeliness_reweighting(mem_ages)
            
            loss_replay = (mse_per_sample * weights).mean()


        # Current Loss + nu * Replay Loss
        total_loss = loss_curr + self.nu * loss_replay
      
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()
        
        self.update_ema_variables(self.model, self.model_ema, self.alpha)
        
        return outputs

    def update_ema_variables(self, model, ema_model, alpha):
        for param, ema_param in zip(model.parameters(), ema_model.parameters()):
            ema_param.data.mul_(alpha).add_(param.data, alpha=1 - alpha)
            
    def reset(self):
        pass

def collect_params(model):
    params = []
    names = []
    for nm, m in model.named_modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm, nn.Conv2d)):
            for np, p in m.named_parameters():
                if np in ['weight', 'bias']:
                    params.append(p)
                    names.append(f"{nm}.{np}")
    return params, names

def configure_model(model):
    model.train()
    model.requires_grad_(False)
    for m in model.modules():
        if isinstance(m, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):
            m.requires_grad_(True)
            m.track_running_stats = False 
        elif isinstance(m, nn.BatchNorm1d):
            m.train()
            m.requires_grad_(True)
        elif isinstance(m, nn.Conv2d):
            m.train()
            m.requires_grad_(True)
    return model

def prepare_model_and_optimizer(model, lr):
    model = configure_model(model)
    params, param_names = collect_params(model)
    optimizer = optim.Adam(params, lr=lr, betas=(0.9, 0.999))
    return model, optimizer

def setup(model, args):
    # args 可以在这里解析超参数
    return RoTTA(model)
